{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача А: определение релевантности вопроса\n",
    "В данной задаче участникам необходимо построить алгоритм, определяющий релевантность поставленных вопросов к параграфу текста. Для решения этой задачи требуется не только понимать, относится ли вопрос к параграфу, но и насколько корректно он поставлен.\n",
    "\n",
    "Это задача бинарной классификации, в которой целевая переменная `target` принимает два значения: 0 и 1. Классу 1 соответствуют релевантные вопросы, заданные к параграфу человеком. К классу 0 относятся вопросы, либо заданные человеком к другим параграфам, либо были составлены компьютером. В качестве целевой метрики используется `ROC-AUC`.\n",
    "\n",
    "Для решения задачи А участникам дается два файла:\n",
    "\n",
    "1. Тренировочные 119 399 пар вопросов и параграфов `train_taskA.csv`, имеющие вид: `paragraph_id`, `question_id`, `paragraph`, `question`, `target`.\n",
    "2. Тестовые 74 295 пар вопросов и параграфов `test_taskA.csv`, имеющие вид: `paragraph_id`, `question_id`, `paragraph`, `question`.\n",
    "\n",
    "В предоставленных тренировочных и тестовых данных релевантные вопросы класса 1 были случайно выбраны из собранных вопросов и ответов. Нерелевантные примеры класса 0, составленные человеком, были получены случайным выбором вопроса к другому параграфу по той же теме. Нерелевантные вопросы класса 0, заданные компьютером, в тренировочных данных отсутствуют. Участникам необходимо самим генерировать такие вопросы для достижения лучшего качества. Также, несмотря на то, что целевая переменная target принимает два значения 0 и 1, в качестве предсказаний можно отправлять вещественные числа.\n",
    "\n",
    "Решением задачи является `.csv` файл на основе `test_taskA.csv`, с заполненным полем `target`. Файл с решением задачи должен иметь следующий вид: `paragraph_id`, `question_id`, `target`.\n",
    "\n",
    "[Пример решения на Python](http://nbviewer.jupyter.org/github/sberbank-ai/data-science-journey-2017/blob/master/taskA/baseline.ipynb \"Ссылка на nbviewer\")\n",
    "\n",
    "[Описание метрики ROC-AUC](http://www.machinelearning.ru/wiki/index.php?title=ROC-%D0%BA%D1%80%D0%B8%D0%B2%D0%B0%D1%8F \"www.machinelearning.ru\")\n",
    "\n",
    "[Материалы соревнования](https://github.com/sberbank-ai/data-science-journey-2017 \"GitHub\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим необходимые бибилиотеки:\n",
    "library(dstools)\n",
    "library(data.table)\n",
    "library(tidyverse)\n",
    "library(magrittr)\n",
    "library(stringr)\n",
    "library(tm)\n",
    "library(text2vec)\n",
    "library(xgboost)\n",
    "library(lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считаем данные:\n",
    "train.data <- readr::read_csv('data/train_task1_latest.csv')\n",
    "test.data <- readr::read_csv('data/test_task1_latest.csv')\n",
    "sample.submsission <- readr::read_csv('data/sample_submission_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Взглянем на данные, чтобы проверить как они загрузились:\n",
    "glimpse(train.data)\n",
    "glimpse(test.data)\n",
    "glimpse(sample.submsission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Баланс классов\n",
    "mean(train.data$target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Посмотрим на распределение количества вопросов в трейне и тесте по параграфам:\n",
    "group.train <- train.data %>% \n",
    "  group_by(paragraph_id)\n",
    "group.test <- test.data %>% \n",
    "  group_by(paragraph_id)\n",
    "\n",
    "ggplot() +\n",
    "  geom_density(data = group.train, mapping = aes(paragraph_id, fill = 'train'), alpha = 1/2) +\n",
    "  geom_density(data = group.test, mapping = aes(paragraph_id, fill = 'test'), alpha = 1/2) +\n",
    "  scale_fill_manual(values = c('train' = 'blue', 'test' = 'red'), name = 'Densities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, id параграфов пересекаются в трейне и в тесте. В дальнейшем можно будет попробовать либо использовать как категориальную фичу, либо учить модель только на пересекающихся парграфах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим столбцы с длиной вопроса и параграфа и посмотрим, как их отношение влияет на `target`:\n",
    "train.data %<>%\n",
    "  mutate(par_len = nchar(paragraph), \n",
    "         ques_len = nchar(question),\n",
    "         len_ratio = ques_len / par_len)\n",
    "test.data %<>%\n",
    "  mutate(par_len = nchar(paragraph), \n",
    "         ques_len = nchar(question),\n",
    "         len_ratio = ques_len / par_len)\n",
    "train.data %>% \n",
    "  group_by(target) %>% \n",
    "  summarise(mean_ratio = mean(len_ratio))\n",
    "rm(group.train, group.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обрабатываем текст. Удаляем стоп-слова и цифры, приводим к нижнему регистру, делаем стемминг:\n",
    "text_modify <- function(txt_, sw_ = c(), stem_ = FALSE) {\n",
    "  if (length(sw_) > 0) {\n",
    "    txt_ %<>% removeWords(sw_)\n",
    "  }\n",
    "  \n",
    "  txt_ %<>% str_to_lower() %>% \n",
    "    str_replace_all('ё', 'е') %>%\n",
    "    str_replace_all('-', ' ') %>% \n",
    "    str_replace_all('\\\\(', ' ( ') %>% \n",
    "    str_replace_all('\\\\)', ' ) ') %>% \n",
    "    str_replace_all('[:digit:]', ' ') %>% \n",
    "    str_replace_all(\"[^[:alpha:]]\", \" \") %>% \n",
    "    removePunctuation() %>% \n",
    "    str_replace_all(\"\\\\s+\", ' ')\n",
    "  \n",
    "  if (stem_ == TRUE) {\n",
    "    # позже разобраться с кодировкой\n",
    "    # txt_ %<>%  enc2utf8() %>% \n",
    "    #   system(command = 'mystem/mystem.exe -cl',\n",
    "    #          intern = TRUE,\n",
    "    #          ignore.stdout = FALSE,\n",
    "    #          ignore.stderr = FALSE,\n",
    "    #          wait = TRUE,\n",
    "    #          input = .,\n",
    "    #          show.output.on.console = TRUE,\n",
    "    #          minimized = FALSE,\n",
    "    #          invisible = TRUE) %>% \n",
    "    #   str_replace_all('[{}]', '') %>%\n",
    "    #   str_replace_all('(\\\\|[^ ]+)', '') %>%\n",
    "    #   str_replace_all('\\\\?', '') %>%\n",
    "    #   str_replace_all('\\\\s+', ' ')\n",
    "    txt_ %<>%\n",
    "      stemDocument('russian')\n",
    "  }\n",
    "  \n",
    "  return(txt_)\n",
    "}\n",
    "\n",
    "sw.url <- 'https://raw.githubusercontent.com/stopwords-iso/stopwords-ru/master/stopwords-ru.txt'\n",
    "sw <- readr::read_csv(sw.url, col_names = F)$X1\n",
    "\n",
    "train.data$paragraph[1] # оригинал\n",
    "text_modify(train.data$paragraph[1]) # обработка\n",
    "text_modify(train.data$paragraph[1], sw) # обработка и удаление стоп слов\n",
    "text_modify(train.data$paragraph[1], sw, T) # обработка, удаление стоп слов и стемминг\n",
    "\n",
    "# отмодифим текстовые данные с полной обработкой\n",
    "train.data$paragraph %<>% text_modify(sw, T)\n",
    "train.data$question %<>% text_modify(sw, T)\n",
    "test.data$paragraph %<>% text_modify(sw, T)\n",
    "test.data$question %<>% text_modify(sw, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим фичи как поиск подстроки вопроса в параграфе, от 1 до 5 слов в группе:\n",
    "subst <- function(paragraph_, question_, n = 2, ret = 'bool') {\n",
    "  stopifnot(ret %in% c('bool', 'int', 'sum'))\n",
    "  stopifnot(n %in% 1:5)\n",
    "  if (n == 1) {\n",
    "    q.split <- str_split(question_, ' ')[[1]]\n",
    "    p.split <- str_split(paragraph_, ' ')[[1]]\n",
    "    \n",
    "  }\n",
    "  if (n == 2) {\n",
    "    q.split <- str_split(question_, ' ')[[1]]\n",
    "    q.split <- paste(q.split, lead(q.split)) \n",
    "    p.split <- str_split(paragraph_, ' ')[[1]]\n",
    "    p.split <- paste(p.split, lead(p.split)) \n",
    "  }\n",
    "  if (n == 3) {\n",
    "    q.split <- str_split(question_, ' ')[[1]]\n",
    "    q.split <- paste(q.split, lead(q.split), lead(q.split, 2)) \n",
    "    p.split <- str_split(paragraph_, ' ')[[1]]\n",
    "    p.split <- paste(p.split, lead(p.split), lead(p.split, 2)) \n",
    "  }\n",
    "  if (n == 4) {\n",
    "    q.split <- str_split(question_, ' ')[[1]]\n",
    "    q.split <- paste(q.split, lead(q.split), lead(q.split, 2), lead(q.split, 3)) \n",
    "    p.split <- str_split(paragraph_, ' ')[[1]]\n",
    "    p.split <- paste(p.split, lead(p.split), lead(p.split, 2), lead(p.split, 3)) \n",
    "  }\n",
    "  if (n == 5) {\n",
    "    q.split <- str_split(question_, ' ')[[1]]\n",
    "    q.split <- paste(q.split, lead(q.split), lead(q.split, 2), lead(q.split, 3), lead(q.split, 4)) \n",
    "    p.split <- str_split(paragraph_, ' ')[[1]]\n",
    "    p.split <- paste(p.split, lead(p.split), lead(p.split, 2), lead(p.split, 3), lead(p.split, 4)) \n",
    "  }\n",
    "  \n",
    "  if (ret == 'bool') return(any(q.split %in% p.split))\n",
    "  if (ret == 'sum') return(sum(q.split %in% p.split))\n",
    "  if (ret == 'int') return(as.integer(any(q.split %in% p.split)))\n",
    "}\n",
    "\n",
    "# количество слов в параграфе и вопросе\n",
    "train.data %<>% \n",
    "  mutate(par_words = sapply(str_split(paragraph, ' '), length))\n",
    "train.data %<>% \n",
    "  mutate(que_words = sapply(str_split(question, ' '), length))\n",
    "test.data %<>% \n",
    "  mutate(par_words = sapply(str_split(paragraph, ' '), length))\n",
    "test.data %<>% \n",
    "  mutate(que_words = sapply(str_split(question, ' '), length))\n",
    "\n",
    "# количество пересекающихся слов\n",
    "train.data$inter_words <- train.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 1, 'sum')\n",
    "  })\n",
    "test.data$inter_words <- test.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 1, 'sum')\n",
    "  })\n",
    "\n",
    "# пересекающиеся пары слов\n",
    "train.data$inter2 <- train.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 2, 'bool')\n",
    "  })\n",
    "test.data$inter2 <- test.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 2, 'bool')\n",
    "  })\n",
    "\n",
    "# пересекающиеся тройки слов\n",
    "train.data$inter3 <- train.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 3, 'bool')\n",
    "  })\n",
    "test.data$inter3 <- test.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 3, 'bool')\n",
    "  })\n",
    "\n",
    "# пересекающиеся четверки слов\n",
    "train.data$inter4 <- train.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 4, 'bool')\n",
    "  })\n",
    "test.data$inter4 <- test.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 4, 'bool')\n",
    "  })\n",
    "\n",
    "# пересекающиеся пятерки слов\n",
    "train.data$inter5 <- train.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 5, 'bool')\n",
    "  })\n",
    "test.data$inter5 <- test.data %>% \n",
    "  select(paragraph, question) %>% \n",
    "  apply(1, function(x){\n",
    "    subst(x[1], x[2], 5, 'bool')\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим как фичи расстояния и похожесть, подсчитанные разными методами:\n",
    "# train\n",
    "it.paragraph <- itoken(train.data$paragraph, progressbar = FALSE)\n",
    "it.question <- itoken(train.data$question, progressbar = FALSE)\n",
    "\n",
    "it <- itoken(c(train.data$paragraph, train.data$question), progressbar = FALSE)\n",
    "v <- create_vocabulary(it) %>% prune_vocabulary(doc_proportion_max = 0.1, term_count_min = 5)\n",
    "vectorizer <- vocab_vectorizer(v)\n",
    "\n",
    "dtm.paragraph <- create_dtm(it.paragraph, vectorizer)\n",
    "dtm.question <- create_dtm(it.question, vectorizer)\n",
    "\n",
    "train.data$jac_sim <- psim2(dtm.paragraph, dtm.question, method = 'jaccard', norm = 'none')\n",
    "train.data$cos_sim <- psim2(dtm.paragraph, dtm.question, method = 'cosine', norm = 'none')\n",
    "#train.data$jac_dist <- pdist2(dtm.paragraph, dtm.question, method = 'jaccard', norm = 'none')\n",
    "#train.data$cos_dist <- pdist2(dtm.paragraph, dtm.question, method = 'cosine', norm = 'none')\n",
    "\n",
    "tfidf <- TfIdf$new()\n",
    "dtm.tfidf.par <- fit_transform(dtm.paragraph, tfidf)\n",
    "tfidf <- TfIdf$new()\n",
    "dtm.tfidf.que <- fit_transform(dtm.question, tfidf)\n",
    "train.data$tfidf_sim <- psim2(dtm.tfidf.par, dtm.tfidf.que, method = 'cosine', norm = 'none')\n",
    "\n",
    "# lsa <- LSA$new(n_topics = 100)\n",
    "# dtm.tfidf.par.lsa <- fit_transform(dtm.tfidf.par, lsa)\n",
    "# lsa <- LSA$new(n_topics = 100)\n",
    "# dtm.tfidf.que.lsa <- fit_transform(dtm.tfidf.que, lsa)\n",
    "# train.data$tfidf_lsa_sim <- psim2(dtm.tfidf.par.lsa, dtm.tfidf.que.lsa, method = 'cosine', norm = 'none')\n",
    "\n",
    "# test\n",
    "it.paragraph <- itoken(test.data$paragraph, progressbar = FALSE)\n",
    "it.question <- itoken(test.data$question, progressbar = FALSE)\n",
    "\n",
    "it <- itoken(c(test.data$paragraph, test.data$question), progressbar = FALSE)\n",
    "v <- create_vocabulary(it) %>% prune_vocabulary(doc_proportion_max = 0.1, term_count_min = 5)\n",
    "vectorizer <- vocab_vectorizer(v)\n",
    "\n",
    "dtm.paragraph <- create_dtm(it.paragraph, vectorizer)\n",
    "dtm.question <- create_dtm(it.question, vectorizer)\n",
    "\n",
    "test.data$jac_sim <- psim2(dtm.paragraph, dtm.question, method = 'jaccard', norm = 'none')\n",
    "test.data$cos_sim <- psim2(dtm.paragraph, dtm.question, method = 'cosine', norm = 'none')\n",
    "#test.data$jac_dist <- pdist2(dtm.paragraph, dtm.question, method = 'jaccard', norm = 'none')\n",
    "#test.data$cos_dist <- pdist2(dtm.paragraph, dtm.question, method = 'cosine', norm = 'none')\n",
    "\n",
    "tfidf <- TfIdf$new()\n",
    "dtm.tfidf.par <- fit_transform(dtm.paragraph, tfidf)\n",
    "tfidf <- TfIdf$new()\n",
    "dtm.tfidf.que <- fit_transform(dtm.question, tfidf)\n",
    "test.data$tfidf_sim <- psim2(dtm.tfidf.par, dtm.tfidf.que, method = 'cosine', norm = 'none')\n",
    "\n",
    "# lsa <- LSA$new(n_topics = 100)\n",
    "# dtm.tfidf.par.lsa <- fit_transform(dtm.tfidf.par, lsa)\n",
    "# lsa <- LSA$new(n_topics = 100)\n",
    "# dtm.tfidf.que.lsa <- fit_transform(dtm.tfidf.que, lsa)\n",
    "# test.data$tfidf_lsa_sim <- psim2(dtm.tfidf.par.lsa, dtm.tfidf.que.lsa, method = 'cosine', norm = 'none')\n",
    "\n",
    "rm(v, dtm.paragraph, dtm.question, it, it.paragraph, it.question, sw, sw.url, text_modify, vectorizer,\n",
    "   tfidf, dtm.tfidf.par, dtm.tfidf.que, lsa, dtm.tfidf.par.lsa, dtm.tfidf.que.lsa, subst)\n",
    "gc()\n",
    "\n",
    "readr::write_rds(train.data, 'data/train.rds')\n",
    "readr::write_rds(test.data, 'data/test.rds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### загрузим rds и обучим модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим данные\n",
    "train.data <- readr::read_rds('data/train.rds')\n",
    "test.data <- readr::read_rds('data/test.rds')\n",
    "sample.submsission <- readr::read_csv('data/sample_submission_a.csv')\n",
    "train.data %<>% filter(paragraph_id <= max(test.data$paragraph_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим данные для построения модели:\n",
    "#1\n",
    "X1 <- rbind(filter(train.data, target == 1), \n",
    "            sample_n(filter(train.data, target == 0), \n",
    "                     nrow(filter(train.data, target == 1))))\n",
    "X1 <- X1[sample(nrow(X1)),]\n",
    "y1 <- X1$target\n",
    "X1 <- X1 %>% \n",
    "    select(-question_id, -paragraph, -question, -target)\n",
    "X1$paragraph_id <- as.factor(X1$paragraph_id)\n",
    "X1 <- ds_toSparseMatrix(X1)\n",
    "\n",
    "#2\n",
    "X2 <- rbind(filter(train.data, target == 1), \n",
    "            sample_n(filter(train.data, target == 0), \n",
    "                     nrow(filter(train.data, target == 1))))\n",
    "X2 <- X2[sample(nrow(X2)),]\n",
    "y2 <- X2$target\n",
    "X2 <- X2 %>% \n",
    "    select(-question_id, -paragraph, -question, -target)\n",
    "X2$paragraph_id <- as.factor(X2$paragraph_id)\n",
    "X2 <- ds_toSparseMatrix(X2)\n",
    "\n",
    "#3\n",
    "X3 <- rbind(filter(train.data, target == 1), \n",
    "            sample_n(filter(train.data, target == 0), \n",
    "                     nrow(filter(train.data, target == 1))))\n",
    "X3 <- X3[sample(nrow(X3)),]\n",
    "y3 <- X3$target\n",
    "X3 <- X3 %>% \n",
    "    select(-question_id, -paragraph, -question, -target)\n",
    "X3$paragraph_id <- as.factor(X3$paragraph_id)\n",
    "X3 <- ds_toSparseMatrix(X3)\n",
    "\n",
    "\n",
    "X_pred <- test.data %>% \n",
    "  select(-question_id, -paragraph, -question)\n",
    "X_pred$paragraph_id <- as.factor(X_pred$paragraph_id)\n",
    "X_pred <- ds_toSparseMatrix(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучим 3 модели и усредним\n",
    "# xgboost\n",
    "k <- 32 \n",
    "param <- list(\n",
    "  eta = 0.2/k,\n",
    "  nround = 550*k,\n",
    "  max_depth = 5,\n",
    "  colsample_bytree = 1,\n",
    "  subsample = 1,\n",
    "  min_child_weight = 1,\n",
    "  gamma = 0,\n",
    "  #scale_pos_weight = 2.5,\n",
    "  tree_method = 'auto',\n",
    "  eval_metric = 'auc',\n",
    "  objective = 'binary:logistic'\n",
    ")\n",
    "\n",
    "model1 <- xgboost(\n",
    "  data = X1,\n",
    "  label = y1,\n",
    "  params = param,\n",
    "  boosting = 'dart',\n",
    "  nrounds = param$nround,\n",
    "  print_every_n = 100,\n",
    "  early_stopping_rounds = 100\n",
    ")\n",
    "\n",
    "model2 <- xgboost(\n",
    "  data = X2,\n",
    "  label = y2,\n",
    "  params = param,\n",
    "  boosting = 'dart',\n",
    "  nrounds = param$nround,\n",
    "  print_every_n = 100,\n",
    "  early_stopping_rounds = 100\n",
    ")\n",
    "\n",
    "model3 <- xgboost(\n",
    "  data = X3,\n",
    "  label = y3,\n",
    "  params = param,\n",
    "  boosting = 'dart',\n",
    "  nrounds = param$nround,\n",
    "  print_every_n = 100,\n",
    "  early_stopping_rounds = 100\n",
    ")\n",
    "\n",
    "res <- tibble(p1 = predict(model1, X_pred),\n",
    "              p2 = predict(model2, X_pred),\n",
    "              p3 = predict(model3, X_pred))\n",
    "sample.submsission$prediction <- rowMeans(res)\n",
    "readr::write_csv(sample.submsission, 'data/xgb_bl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучим 3 модели и усредним\n",
    "# lightgbm\n",
    "k <- 4\n",
    "param <- list(learning_rate = 0.2/k,\n",
    "              num_tree = 550*k, \n",
    "              max_depth = 5, \n",
    "              num_leaves = 1024,\n",
    "              max_bin = 256,\n",
    "              lambda_l1 = 3,\n",
    "              lambda_l2 = 5,\n",
    "              feature_fraction = 1, \n",
    "              bagging_fraction = 1, \n",
    "              bagging_freq = 6, \n",
    "              #scale_pos_weight = 2.5, \n",
    "              metric = 'auc',\n",
    "              objective = 'binary')\n",
    "\n",
    "lgb.unloader(wipe = T)\n",
    "model1 <- lightgbm(data = X1,\n",
    "                  label = y1,\n",
    "                  params = param,\n",
    "                  boosting = 'dart',\n",
    "                  nrounds = param$num_tree,\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = 1L)\n",
    "\n",
    "#lgb.unloader(wipe = T)\n",
    "model2 <- lightgbm(data = X2,\n",
    "                  label = y2,\n",
    "                  params = param,\n",
    "                  boosting = 'dart',\n",
    "                  nrounds = param$num_tree,\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = 1L)\n",
    "\n",
    "#lgb.unloader(wipe = T)\n",
    "model3 <- lightgbm(data = X3,\n",
    "                  label = y3,\n",
    "                  params = param,\n",
    "                  boosting = 'dart',\n",
    "                  nrounds = param$num_tree,\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = 1L)\n",
    "\n",
    "res <- tibble(p1 = predict(model1, X_pred),\n",
    "              p2 = predict(model2, X_pred),\n",
    "              p3 = predict(model3, X_pred))\n",
    "sample.submsission$prediction <- rowMeans(res)\n",
    "readr::write_csv(sample.submsission, 'data/lgb_bl.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
